* Lorenz synthetic time series data
This package contains modules for generating synthetic time series data using the Lorenz systemll as well as modules
for using the synthetic time series from a python program.

** Installation
Install the package by running 'python setup.py develop' This will install the package in development mode, linking to
this directory instead of copying the files to your python distribution.

** Storage format
The data is stored in HDF5 files. Each hyper parameter setting of the Lorenz system is collected in its own group in
the HDF5 root. In the hyper parameter group, there's a second level of groups which gathers datasets that share ODE
solver settings, such as time resolution, time to skip and random number seed. In these groups there are pairs datasets,
one with the original data (with the name suffix '-original') generated by the ODE solver with the given settings,
and one is the data projected on canonical principal components (having the suffix '-pca').
The original data has been standardizes using a "canonical" mean and standard deviation calculated from a large sample
of trajectories, these statistics are saved in the file 'data/standardize_statistics.npz' using the keys 'mean' and
'std'. The projection matrix for the PCA projected data is
ordered by the magnitude of the eigenvalues, so the leading dimension correspond to the leading principal component.
The projection matrix is stored in an attribute called 'pca' of the pca projected dataset.
The canonical principal components are located in 'data/standardize_statistics.npz', having the key 'pc'. These are the
principal components which will be used for all projections, and are generated from a large sample of trajectories. We
use a fixed PCA base to make learning on the dimensionality reduced space viable over different trajectories and
different Lorenz systems.

** Generate data
*** Canonical data
The script 'generate_data.sh' in the package root generates data using a fixed random seed, to be useful for replcating
results. The script generates to primal HDF5 datasets, one named 'lorenz.h5' which contains trajectories sampled from
a Lorenz system with the same hyper parameters. The 'random_hp_lorenz.h5' file contains datasets sampled from Lorenz
systems with randomly perturbed hyper parameters. Each of the perturbed system has 100 trajectories.
These datasets are further split into training, validation and test datasets, where the name is prefixed by the split
name (e.g. 'lorenz_train.h5' for the training split of the 'lorenz.h5' dataset). The split ratios are 90% for training
and 5% each for validation and test.

*** Generate your own data
To generate data, run 'bin/generate_lorenz_data.py N', where N is the number of trajectories to generate. By default,
the data is created in the HDF5 file 'data/lorenz.h5'. Run 'bin/generate_lorenz_data.py --help' for a description of
available settings.

** Usage
To use the data, the class Dataset in lorenz/dataset.py provides convenience functions. This class is called with the
file path of the dataset to use. The dataset has a method called 'random_iterator' which return mini-batches with random
trajectories from the dataset. By default, all trajectories will be sampled once.

See the script 'examples/plot_random_batches.py' for a usage example.

